{"word":"pretraining","priority":"★★☆","meanings":[{"part_of_speech":"noun","definition":"事前学習、予備訓練","english_definition":"The process of training a model on a large dataset before fine-tuning it on a specific task.","examples":[{"sentence":"Pretraining a language model on a massive text corpus can significantly improve its performance on downstream tasks.","translation":"大規模なテキストコーパスで言語モデルを事前学習させることは、下流タスクでのパフォーマンスを大幅に向上させることができます。"},{"sentence":"The pretraining phase helps the model learn general features of the data.","translation":"事前学習段階は、モデルがデータの一般的な特徴を学習するのに役立ちます。"}],"collocations":["language model pretraining","self-supervised pretraining","unsupervised pretraining","pretrained model","pretraining data"],"synonyms":["initial training","base training","warm-up training"],"antonyms":["fine-tuning","post-training"]},{"part_of_speech":"verb","definition":"事前学習する、予備訓練する","english_definition":"To train a model on a large dataset before fine-tuning it on a specific task.","examples":[{"sentence":"We pretrain the model on a large dataset of images before applying it to object detection.","translation":"オブジェクト検出に適用する前に、大規模な画像データセットでモデルを事前学習させます。"},{"sentence":"The researchers pretrained their model using a self-supervised learning approach.","translation":"研究者たちは、自己教師あり学習アプローチを用いてモデルを事前学習させました。"}],"collocations":["pretrain a model","pretrain on a dataset","pretrain using self-supervision","pretrain for downstream tasks"],"synonyms":["initially train","base train","warm-up train"],"antonyms":["fine-tune","post-train"]}],"etymology":{"value":"The word \"pretraining\" is formed by combining the prefix \"pre-\" (meaning before) with the word \"training.\" It refers to the initial training phase that occurs before more specific or fine-grained training.","priority":"★★☆"},"pronunciation":{"ipa":"/ˌpriːˈtreɪnɪŋ/","syllables":"pre-train-ing"},"inflection":{"noun_plural":"pretrainings","verb_forms":{"present_simple":["pretrain","pretrains"],"past_simple":"pretrained","past_participle":"pretrained","present_participle":"pretraining"}},"usage_notes":{"explanation":"\"Pretraining\" is commonly used in the field of machine learning, particularly in deep learning. It involves training a model on a large, general dataset to learn useful representations before fine-tuning it on a smaller, more specific dataset for a particular task. This technique helps improve the model's performance and generalization ability, especially when the labeled data for the specific task is limited.","priority":"★★★"},"common_mistakes":{"examples":[{"incorrect":"✗ We are pre-training the model with the data.","correct":"✓ We are pretraining the model on the data.","note":"The correct preposition to use with \"pretraining\" is \"on,\" not \"with.\" You pretrain a model *on* a dataset."},{"incorrect":"✗ The model was pre-trained and then fine tuning.","correct":"✓ The model was pretrained and then fine-tuned.","note":"Ensure that both \"pre-trained\" and \"fine-tuned\" are hyphenated when used as adjectives and that \"fine-tuned\" is correctly spelled."}],"priority":"★★☆"},"related_words":{"derivatives":["pretrained","pretrainer"],"related_terms":["transfer learning","fine-tuning","self-supervised learning","unsupervised learning","deep learning"],"priority":"★★★"},"level_frequency":{"CEFR":"C1","frequency_google_ngram":"Relatively low frequency, but increasing in recent years due to the rise of deep learning.","priority":"★☆☆"},"readability_explanation":{"level":"C1","text":"The term \"pretraining\" is used in machine learning, especially deep learning. It means training a model on a big dataset first, so it can learn general patterns before being trained on a smaller, specific dataset for a certain task. This helps the model work better, especially when there isn't much specific data available. It's like giving the model a good foundation before teaching it the details."},"example_sentences":[{"sentence":"The model achieved state-of-the-art results after **pretraining** on a large corpus of text.","translation":"そのモデルは、大規模なテキストコーパスで**事前学習**を行った後、最先端の結果を達成しました。","type":"noun","meaning_category":"事前学習"},{"sentence":"We **pretrain** the network using a self-supervised objective.","translation":"自己教師ありの目的関数を用いてネットワークを**事前学習**させます。","type":"verb","meaning_category":"事前学習する"},{"sentence":"The benefits of **pretraining** are most evident when the target task has limited labeled data.","translation":"**事前学習**の利点は、ターゲットタスクのラベル付きデータが限られている場合に最も顕著です。","type":"noun","meaning_category":"事前学習"},{"sentence":"By **pretraining** on a related task, the model can learn useful features that transfer to the new task.","translation":"関連タスクで**事前学習**を行うことで、モデルは新しいタスクに転送できる有用な特徴を学習できます。","type":"verb","meaning_category":"事前学習する"}]}