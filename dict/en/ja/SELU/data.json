{"word":"SELU","priority":"Low","meanings":[{"part_of_speech":"Noun","definition":"スケーリングされた指数線形ユニット (Scaled Exponential Linear Unit) の略。深層学習における活性化関数の一種。","english_definition":"An acronym for Scaled Exponential Linear Unit, a type of activation function used in deep learning.","examples":[{"sentence":"SELU活性化関数は、自己正規化特性を持つため、深いニューラルネットワークの学習に役立ちます。","translation":"The SELU activation function is helpful for training deep neural networks because it has self-normalizing properties."}],"collocations":["SELU activation function","deep learning","neural network","self-normalization"],"synonyms":["Scaled Exponential Linear Unit"],"antonyms":[]}],"etymology":{"value":"Scaled Exponential Linear Unitの頭字語","priority":"High"},"pronunciation":{"ipa":"/ˈsɛl.juː/","syllables":"SEL-U"},"inflection":{"noun_plural":"SELUs"},"usage_notes":{"explanation":"SELUは、ニューラルネットワークの活性化関数として使用され、自己正規化特性を持つため、勾配消失や爆発の問題を軽減し、深いネットワークの学習を安定化させることができます。主に畳み込みニューラルネットワークや全結合ニューラルネットワークで使用されます。","priority":"Medium"},"common_mistakes":{"examples":[{"incorrect":"ReLU is better than SELU in all cases.","correct":"SELU can be more effective than ReLU in deep networks due to its self-normalizing properties."}],"priority":"Low"},"related_words":{"derivatives":[],"related_terms":["ReLU","ELU","activation function","deep learning","neural network"]},"level_frequency":{"level":"Advanced","frequency_google_ngram":"Low","priority":"Low"},"readability_explanation":{"level":"Advanced","text":"SELU (スケーリングされた指数線形ユニット)は、深層学習で使用される高度な活性化関数です。自己正規化特性を持ち、深いニューラルネットワークの学習を安定化させるのに役立ちます。この用語は、深層学習の専門家や研究者によって主に使用されます。"},"example_sentences":[{"sentence":"The researchers found that using SELU activation improved the performance of their deep neural network.","translation":"研究者たちは、SELU活性化関数を使用することで、彼らの深いニューラルネットワークの性能が向上することを発見しました。","type":"Noun","meaning_category":"活性化関数"}]}